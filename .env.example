# Optional: Enable full AI chat for Daily Companion.
# Default behavior stays offline + rule-based when not configured.

# Choose one:
#   offline  (default)
#   ollama   (local LLM via http://localhost:11434)
#   openai   (any OpenAI-compatible server exposing /v1/chat/completions)
VITE_AI_MODE=offline

# Ollama example:
# VITE_AI_MODE=ollama
# VITE_AI_BASE_URL=http://localhost:11434
# VITE_AI_MODEL=llama3.1

# OpenAI-compatible example (local gateway or cloud):
# VITE_AI_MODE=openai
# VITE_AI_BASE_URL=https://api.openai.com
# VITE_AI_API_KEY=YOUR_KEY_HERE
# VITE_AI_MODEL=gpt-4o-mini

# Optional tuning:
# VITE_AI_TEMPERATURE=0.6
